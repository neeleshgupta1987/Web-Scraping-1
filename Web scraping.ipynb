{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c135ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Header\n",
      "0                       Main Page\n",
      "1            Welcome to Wikipedia\n",
      "2   From today's featured article\n",
      "3                Did you knowÂ ...\n",
      "4                     In the news\n",
      "5                     On this day\n",
      "6      From today's featured list\n",
      "7        Today's featured picture\n",
      "8        Other areas of Wikipedia\n",
      "9     Wikipedia's sister projects\n",
      "10            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "response = requests.get(url)\n",
    "html_content = response.content\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "header_tags = soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "header_texts = [tag.get_text() for tag in header_tags]\n",
    "data = {'Header': header_texts}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f097019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams:\n",
      "  Rank              Team Matches Points Rating\n",
      "0    1    Australia\\nAUS      23  2,714    118\n",
      "1    2     Pakistan\\nPAK      20  2,316    116\n",
      "2    3        India\\nIND      36  4,081    113\n",
      "3    4   New Zealand\\nNZ      27  2,806    104\n",
      "4    5      England\\nENG      24  2,426    101\n",
      "5    6  South Africa\\nSA      19  1,910    101\n",
      "6    7   Bangladesh\\nBAN      28  2,661     95\n",
      "7    8  Afghanistan\\nAFG      16  1,404     88\n",
      "8    9     Sri Lanka\\nSL      32  2,794     87\n",
      "9   10   West Indies\\nWI      38  2,582     68\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url_teams = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "response_teams = requests.get(url_teams)\n",
    "soup_teams = BeautifulSoup(response_teams.content, \"html.parser\")\n",
    "teams_table = soup_teams.find(\"table\", class_=\"table\")\n",
    "teams_data = []\n",
    "for row in teams_table.find_all(\"tr\")[1:]:\n",
    "    cells = row.find_all(\"td\")\n",
    "    team_rank = cells[0].text.strip()\n",
    "    team_name = cells[1].text.strip()\n",
    "    team_matches = cells[2].text.strip()\n",
    "    team_points = cells[3].text.strip()\n",
    "    team_rating = cells[4].text.strip()\n",
    "    teams_data.append([team_rank, team_name, team_matches, team_points, team_rating])\n",
    "teams_df = pd.DataFrame(teams_data, columns=[\"Rank\", \"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "print(\"Top 10 ODI Teams:\")\n",
    "print(teams_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f77f51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTION 4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19d848bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89738eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(headers, data):\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17e4d372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_teams():\n",
    "    url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "    soup = scrape_data(url)\n",
    "\n",
    "    headers = [\"Rank\", \"Team\", \"Matches\", \"Points\", \"Rating\"]\n",
    "    data = []\n",
    "\n",
    "    team_rows = soup.select(\".table-body tr\")\n",
    "    for row in team_rows:\n",
    "        cols = row.select(\"td\")\n",
    "        rank = cols[0].text.strip()\n",
    "        team = cols[1].text.strip()\n",
    "        matches = cols[2].text.strip()\n",
    "        points = cols[3].text.strip()\n",
    "        rating = cols[4].text.strip()\n",
    "        data.append([rank, team, matches, points, rating])\n",
    "\n",
    "    return create_dataframe(headers, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ff88493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_batting_players():\n",
    "    url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "    soup = scrape_data(url)\n",
    "\n",
    "    headers = [\"Rank\", \"Player\", \"Team\", \"Rating\"]\n",
    "    data = []\n",
    "\n",
    "    player_rows = soup.select(\".table-body tr\")\n",
    "    for row in player_rows:\n",
    "        cols = row.select(\"td\")\n",
    "        rank = cols[0].text.strip()\n",
    "        player = cols[1].text.strip()\n",
    "        team = cols[2].text.strip()\n",
    "        rating = cols[4].text.strip()\n",
    "        data.append([rank, player, team, rating])\n",
    "\n",
    "    return create_dataframe(headers, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82ffa062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams:\n",
      "Empty DataFrame\n",
      "Columns: [Rank, Team, Matches, Points, Rating]\n",
      "Index: []\n",
      "\n",
      "Top 10 Women's ODI Batting Players:\n",
      "Empty DataFrame\n",
      "Columns: [Rank, Player, Team, Rating]\n",
      "Index: []\n",
      "\n",
      "Top 10 Women's ODI All-rounders:\n",
      "Empty DataFrame\n",
      "Columns: [Rank, Player, Team, Rating]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "def top_10_allrounders():\n",
    "    url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "    soup = scrape_data(url)\n",
    "\n",
    "    headers = [\"Rank\", \"Player\", \"Team\", \"Rating\"]\n",
    "    data = []\n",
    "\n",
    "    player_rows = soup.select(\".table-body tr\")\n",
    "    for row in player_rows:\n",
    "        cols = row.select(\"td\")\n",
    "        rank = cols[0].text.strip()\n",
    "        player = cols[1].text.strip()\n",
    "        team = cols[2].text.strip()\n",
    "        rating = cols[4].text.strip()\n",
    "        data.append([rank, player, team, rating])\n",
    "\n",
    "    return create_dataframe(headers, data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    top_teams_df = top_10_teams()\n",
    "    top_batting_players_df = top_10_batting_players()\n",
    "    top_allrounders_df = top_10_allrounders()\n",
    "\n",
    "    print(\"Top 10 ODI Teams:\")\n",
    "    print(top_teams_df)\n",
    "    print(\"\\nTop 10 Women's ODI Batting Players:\")\n",
    "    print(top_batting_players_df)\n",
    "    print(\"\\nTop 10 Women's ODI All-rounders:\")\n",
    "    print(top_allrounders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee5efa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Headline, Time, News Link]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 5\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the news page\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all news headlines, times, and links\n",
    "headlines = [headline.get_text() for headline in soup.find_all(\"div\", class_=\"Card-title\")]\n",
    "times = [time.get_text() for time in soup.find_all(\"div\", class_=\"Card-time\")]\n",
    "links = [link.a[\"href\"] for link in soup.find_all(\"div\", class_=\"Card-title\")]\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\"Headline\": headlines, \"Time\": times, \"News Link\": links}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "450892ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 6\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "response = requests.get(url)\n",
    "content = response.content\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "articles = soup.find_all(\"li\", class_=\"js-article-list-item\")\n",
    "paper_titles = []\n",
    "authors_list = []\n",
    "published_dates = []\n",
    "paper_urls = []\n",
    "for article in articles:\n",
    "    paper_title = article.find(\"h2\", class_=\"pod-article-title\").text.strip()\n",
    "    authors = [author.text.strip() for author in article.find_all(\"span\", class_=\"pod-authors__name\")]\n",
    "    published_date = article.find(\"span\", class_=\"js-article-history-date\").text.strip()\n",
    "    paper_url = article.find(\"a\", class_=\"pod-article-link\")[\"href\"]\n",
    "    \n",
    "    paper_titles.append(paper_title)\n",
    "    authors_list.append(authors)\n",
    "    published_dates.append(published_date)\n",
    "    paper_urls.append(paper_url)\n",
    "data = {\n",
    "    \"Paper Title\": paper_titles,\n",
    "    \"Authors\": authors_list,\n",
    "    \"Published Date\": published_dates,\n",
    "    \"Paper URL\": paper_urls\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b790511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Restaurant Name, Cuisine, Location, Ratings, Image URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# QUESTION 7\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url = \"https://www.dineout.co.in/\"\n",
    "response=requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "restaurant_names = []\n",
    "cuisines = []\n",
    "locations = []\n",
    "ratings = []\n",
    "image_urls = []\n",
    "restaurant_cards = soup.find_all(\"div\", class_=\"restaurant-card\")\n",
    "for card in restaurant_cards:\n",
    "    # Restaurant name\n",
    "    restaurant_name = card.find(\"h2\", class_=\"restnt-name\").text.strip()\n",
    "    restaurant_names.append(restaurant_name)\n",
    "\n",
    "    # Cuisine\n",
    "    cuisine = card.find(\"p\", class_=\"restnt-cuisine\").text.strip()\n",
    "    cuisines.append(cuisine)\n",
    "\n",
    "    # Location\n",
    "    location = card.find(\"span\", class_=\"restnt-loc\").text.strip()\n",
    "    locations.append(location)\n",
    "\n",
    "    # Ratings\n",
    "    rating = card.find(\"span\", class_=\"rating\").text.strip()\n",
    "    ratings.append(rating)\n",
    "\n",
    "    # Image URL\n",
    "    img_url = card.find(\"div\", class_=\"restnt-thumbnail\").img[\"src\"]\n",
    "    image_urls.append(img_url)\n",
    "data = {\n",
    "    \"Restaurant Name\": restaurant_names,\n",
    "    \"Cuisine\": cuisines,\n",
    "    \"Location\": locations,\n",
    "    \"Ratings\": ratings,\n",
    "    \"Image URL\": image_urls\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
